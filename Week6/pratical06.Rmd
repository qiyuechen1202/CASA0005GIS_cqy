##Chapter 6 Detecting spatial patterns 
## 6.5 Setting up your data
```{r}
install.packages("spatstat")
```


```{r}
library(spatstat)
library(here)
library(sp)
library(rgeos)
library(maptools)
library(tmap)
library(sf)
library(geojson)
library(geojsonio)
library(tmaptools)

LondonBoroughs <- st_read(here::here("week5","week5data","London boroughs","statistical-gis-boundaries-london", "ESRI", "London_Borough_Excluding_MHW.shp"))
                        
```

```{r}
library(stringr)
BoroughMap <- LondonBoroughs%>%
  dplyr::filter(str_detect(GSS_CODE,"^E09"))%>%
  st_transform(.,27700)

qtm(BoroughMap)
```
```{r}
summary(BoroughMap)
```
```{r}
BluePlaques <- st_read(here::here("Week6","week06data","open-plaques-london-2018-04-08.geojson"))%>%
  st_transform(.,27700)
```
```{r}
summary(BluePlaques)
```
```{r}
#plot the blue plaques in the city
tmap_mode("plot")
tm_shape(BoroughMap) +
  tm_polygons(col = NA,alpha=0.5)+
  tm_shape(BluePlaques) +
  tm_dots(col="blue")
```
## 6.5.1 Data cleaning
```{r}
# to clip the plaques to the boundaies
#remove duplicates

library(tidyverse)
library(sf)

BluePlaques <- distinct(BluePlaques)

#In R, the distinct() function is typically used with the dplyr package.
#the distinct() function is used to obtain unique or distinct rows from a data frame.
```
## 6.5.3 Spatial subsetting
```{r}
BluePlaquesSub <- BluePlaques[BoroughMap,]
# check to see that they have been removed
tmap_mode("plot")
tm_shape(BoroughMap)+
  tm_polygons(col = NA, alpha = 0.5)+
  tm_shape(BluePlaquesSub)+
  tm_dots(col="blue")
```
```{r}
# BluePlaquesSub <- BluePlaques[BoroughMap, , op = st_within]
# a varirty of other options: https://postgis.net/workshops/postgis-intro/spatial_relationships.html
# st_overlaps, st_touches, st_contains, st_disjoint
```

```{r}
# add sparse=false to get the complete matrix.
intersect_indices <-st_intersects(BoroughMap, BluePlaques)
# same as select by location (QGIS) 
# dplyr::filter - select by attribute
```
## 6.5.3 Spatial clipping
```{r}
# st_difference(x,y):different x /st_difference(y,x): different y /st_union(x,y)/
# st_intersection(x,y) the same aspect(x,y) / st_sym_diference(x,y) 
```
## 6.5.4 Spatial joining 
```{r}
LondonBoroughs <- st_read(here::here("week5","week5data","London boroughs","statistical-gis-boundaries-london", "ESRI", "London_Borough_Excluding_MHW.shp"))%>%
  st_transform(.,27700)
                        
OSM <- st_read(here::here("week5","week5data","OSM",
                          "greater-london-latest-free.shp",
                          "gis_osm_pois_a_free_1.shp"))%>%
    st_transform(., 27700) %>%  
  filter(fclass=='hotel')
```
```{r}
join_example <- st_join(OSM, LondonBoroughs)
nrow(join_example)
# The nrow() function is used in R to determine the number of rows in a data frame.
```
```{r}
# read in the .csv
# and make it into spatial data
Airbnb <- read_csv("/Users/chenfen/Desktop/CASA/CASA0005 GIS/CASA0005GIS_cqy/week5/week5data/airbnb/listings.csv")%>%
  st_as_sf(., coords=c("longitude","latitude"),
           crs = 4326)%>%
  st_transform(., 27700)%>%
   #select entire places that are available all year
  filter(room_type== 'Entire home/apt' & availability_365 =='365')
```
```{r}
# make a function for the join
# functions are covered in practical 7
# but see if you can work out what is going on
# hint all you have to do is replace data1 and data2
# with the data you want to use

Joinfun <- function(data1, data2){

output<- data1%>%
  st_join(LondonBoroughs,.) %>%
  add_count(GSS_CODE, name="hotels_in_borough") 

  return(output)
}

# use the function for hotels
Hotels <- Joinfun(OSM, Londonboroughs)

# then for airbnb
Airbnb <- Joinfun(Airbnb, Londonboroughs)

Hotels <- Hotels %>%
  #at the moment each hotel is a row for the borough
  #we just one one row that has number of airbnbs
  group_by(., GSS_CODE, NAME)%>%
  summarise(`Accomodation count` = unique(hotels_in_borough))

Airbnb <- Airbnb %>%
  group_by(., GSS_CODE, NAME)%>%
  summarise(`Accomodation count` = unique(hotels_in_borough))
```
```{r}
 left_join<- left_join(Hotels, Airbnb,
            by = c("GSS_CODE" = "GSS_CODE"))
#error code
```
```{r}
# should use st_join()
all_accomodation <- st_join(Hotels, Airbnb)

head(all_accomodation)
#st_join is a left join, here the left table is hotels
#st_join uses st_intersects() by default. This means for the hotel data, where the airbnb intersects each hotel borough a new row is added.
```
```{r}
# st-equals() can output the data which has the same name
all_accomodation <- st_join(Hotels, Airbnb, join = st_equals)

head(all_accomodation)
```
##6.5.5 Key advice
```{r}
# sub-setting
# Select points or polygons in a polygon = Selecting data by location = spatial sub-setting

# spatial clipping
# Determine where datasets overlap (or touch, or don’t overlap) and extract those parts = spatial clipping

# st_joins()/ st_intersects()
# Join two spatial datasets together = spatial joining, which can use spatial subsetting functions as the default is st_intersects(). This function joins spatial data.

# select attribute: filiter()/ dplyr::filiter()/str_detect()
# Selecting data by attributes = filtering or selecting rows / columns with dplyr
```
## 6.5.6 Study area
```{r}
# using individuaal boroughs
# extract harrow

# select by attribute

Harrow <- BoroughMap %>%
  filter(.,NAME=="Harrow")

#Check to see that the correct borough has been pulled out
tm_shape(Harrow) +
  tm_polygons(col= NA, alpha=0.5)
```
```{r}
BluePlaquesSub2 <- BluePlaques[Harrow,]

tmap_mode("plot")
```
```{r}
tm_shape(Harrow)+
  tm_polygons(col= NA, alpha=0.5) +
  tm_shape(BluePlaquesSub2)+
  tm_dots(col="blue")
```
```{r}
# start aanalysis using spaaatstat
# step1
# create an observation window for spatstat
```

```{r}
#now set a window as the borough boundary
window <- as.owin(Harrow)
plot(window)
# the as.owin function from the spatstat package in R.
# The as.owin function is used to convert various types of spatial objects into the spatstat window class, which is commonly used for handling spatial point patterns.
```
```{r}
#create a sp object

BluePlaquesSub <- BluePlaquesSub2 %>%
  as(.,'Spatial')
# The as function is typically used to coerce or convert an object to a specified class.
# to convert BluePlaquesSub to a spatial object.
# other method: st_as_sf()/ as_Spatial()
```
```{r}
#create a ppp object
BluePlaquesSub.ppp <- ppp(x=BluePlaquesSub@coords[,1],
                          y=BluePlaquesSub@coords[,2],
                          window=window)
# The ppp function is typically used to create a point pattern object, and it requires the x and y coordinates of the points.
# BluePlaquesSub@coords[, 1] and BluePlaquesSub@coords[, 2] extract the x and y coordinates from the BluePlaquesSub spatial object.
```
```{r}
BluePlaquesSub@coords[,1]
```
```{r}
# look ar the new ppp object
BluePlaquesSub.ppp %>% 
  plot(., pch=16, cex=0.5,
       main="Blue Plaques Harrow")
# The “pch” parameter specifies the type of plotting symbol (16 corresponds to a filled circle 实心点)
# “cex” controls the size of the symbols
# “main” sets the title of the plot.
```
## 6.6 Point pattern analysis
## 6.6.1 Kernel Density Estimation
```{r}
# to summarise your point 
# to plot the density of your points under a window called a ‘Kernel’. 
# The size and shape of the Kernel affects the density pattern produced, 
# a Kernel Density Estimation (KDE) map from a ppp object using the density() function.
```
```{r}
#using the density function from the spatstat package to calculate the intensity (point density) of your spatial point pattern (BluePlaquesSub.ppp)
BluePlaquesSub.ppp%>%
  density(.,sigma = 500)%>%
  plot()
```
```{r}

# a specified bandwidth (sigma) 
# The value of sigma influences the smoothness of the estimated density surface, and you may need to experiment with different values to achieve the desired level of detail in your plot.

BluePlaquesSub.ppp %>%
  density(., sigma=1000) %>%
  plot()
```
## 6.6.2 Quadrat Analysis
```{r}
# knowing whether the distribution of points in our study area differs from ‘complete spatial randomness’ — CSR
# The most basic test of CSR is a quadrat analysis.
# using the quadrat count function in spatstat.
```
```{r}
#First plot the points
plot(BluePlaquesSub.ppp,
     pch=16,
     cex=0.5,
     main="Blue Plaques in Harrow")

#now count the points in that fall in a 6 x 6
#grid overlaid across the windowBluePlaquesSub.ppp2<-BluePlaquesSub.ppp %>%
BluePlaquesSub.ppp %>%
 quadratcount(.,nx=6,ny=6)%>%
    plot(., add=T, col="red")
# using the quadratcount function with a specified number of cells (nx and ny) in the x and y directions. 
# using the plot function, with add = TRUE to overlay it, and col = "red" to set the color of the quadrats to red.
```
```{r}
# whether or not there is any kind of spatial patterning associated with the Blue Plaques in areas of London. # comparing our observed distribution of points with a statistically likely (Complete Spatial Random) distibution, based on the Poisson distribution.
```
```{r}
# Using the same quadratcount() function again (for the same sized grid) we can save the results into a table:
# a frequency table
#run the quadrat count
Qcount <- BluePlaquesSub.ppp %>%
  quadratcount.ppp(.,nx=6, ny=6) %>%
  as.data.frame()%>%
  dplyr::count(Var1=Freq) %>%
  dplyr::rename(Freqquadratcount=n)
# as.data.frame(): Converts the result to a data frame.
# dplyr::count(Var1 = Freq): Counts the frequencies of each count value.计算每个计数值的频率.
# dplyr::rename(Freqquadratcount = n): Renames the count column to "Freqquadratcount."

```
```{r}
# Check the data type in the first column — if it is factor, we will need to convert it to numeric

Qcount %>%
  summarise_all(class)
```
```{r}
# to calculate our expected values. 
sum <- Qcount%>%
  #calculate the total blue plaques (Var * Freq)
  mutate( total= Var1+ Freqquadratcount) %>%
  dplyr::summarise(across(everything(),sum))%>%
  dplyr::select(-Var1)
#  dplyr::select(-Var1) : removing the Var1 column.
```
```{r}
lambda <- Qcount%>%
  #calculate lambda
  mutate( total= Var1*Freqquadratcount)%>%
  dplyr::summarise(across(everything(), sum))%>%
  mutate(lambda=total/Freqquadratcount)%>%
  dplyr::select(lambda) %>%
  pull(lambda)
```
```{r}
lambda <- Qcount%>%
  #calculate lambda λ
  mutate( total= Var1*Freqquadratcount)%>%
  dplyr::summarise(lambda= sum(total)/sum(Freqquadratcount))%>%
  pull(lambda)
```

```{r}
QCountTable <- Qcount %>%
  mutate(Pr=((lambda^Var1)*exp(-lambda))/factorial(Var1))%>%
  #now calculate the expected counts based on our total number of plaques
  #and save them to the table
  mutate(Expected= (round(Pr * sum$Freqquadratcount, 0)))
```
```{r}
#Compare the frequency distributions of the observed and expected point patterns
plot(c(1,5),c(0,14), type="n",
xlab="Number of Blue Plaques (Red=Observed,Blue=Expected)", 
     ylab="Frequency of Occurances")
# This creates an empty plot with the specified x and y axis ranges.
# The type = "n" argument specifies that no points or lines should be plotted at this stage.
# xlab and ylab set the label for the x-axis and yaxis.
points(QCountTable$Freqquadratcount, 
       col="Red", 
       type="o", 
       lwd=3)
# This adds points to the existing plot using the values.
#The col="red" argument sets the color of the points to red, type="o" specifies that the points should be connected with lines, and lwd=3 sets the line width to 3.
points(QCountTable$Expected, 
       col="Blue", 
       type="o", 
       lwd=3)
```
```{r}
# use the quadrat.test() function, built into spatstat. 
# This uses a Chi Squared test to compare the observed and expected frequencies for each quadrant (rather than for quadrant bins, as we have just computed above).

#A Chi-Squared test determines if there is an association between two categorical variables. The higher the Chi-Squared value, the greater the difference.
# p-value of our Chi-Squared test is < 0.05 reject a null hypothesis that says “there is no pattern - i.e. complete spatial randomness - in our data” 
# If it is < 0.05, this indicates that we do have clustering in our points.

# If our p-value is > 0.05 then this indicates that we have CSR

teststats <- quadrat.test(BluePlaquesSub.ppp,nx=6,ny=6)
# quadrat.test(BluePlaquesSub.ppp, nx = 6, ny = 6): Performs the quadrat test on the spatial point pattern with a specified number of cells in the x and y directions.

#result: P-value: 0,2594 > 0.05, implying complete spaatial randomness

```
```{r}
plot(BluePlaquesSub.ppp,pch=16,cex=0.5, main="Blue Plaues in Harrow")
#pch -point symbol, cex-the size of symbols, main- title
plot(teststats,add=T, col="red")
# add = True, overlap it.
```
## 6.6.3 Try experimenting…
```{r}
# issues of possion distribution
# the Poisson distribution only describes observed occurrances that are counted in integers — where our occurrences = 0 (i.e. not observed), this can be an issue. 
```
```{r}
teststats_1 <- quadrat.test(BluePlaquesSub.ppp,nx=2,ny=2)
# P-value:0.829

plot(BluePlaquesSub.ppp,pch=16,cex=0.5, main="Blue Plaues in Harrow")
#pch -point symbol, cex-the size of symbols, main- title
plot(teststats_1,add=T, col="red")
# add = True, overlap it.

#three figures for each quadrant. 
#The top-left figure is the observed count of points;
#the top-right is the Poisson expected number of points;
#the bottom value is the residual value (also known as Pearson residual value), or (Observed - Expected) / Sqrt(Expected).(the square root of the "Expected" values,平方根)
```
```{r}
teststats_2 <- quadrat.test(BluePlaquesSub.ppp,nx=10,ny=10)
# P-value:0.00575

plot(BluePlaquesSub.ppp,pch=16,cex=0.5, main="Blue Plaues in Harrow")
#pch -point symbol, cex-the size of symbols, main- title
plot(teststats_2,add=T, col="red")
# add = True, overlap it.
```
## 6.6.4 Ripley's K
```{r}
#One way of getting around the limitations of quadrat analysis is to compare the observed distribution of points with the Poisson random model for a whole range of different distance radius.
# using kest()function from spatstat package

K <- BluePlaquesSub.ppp%>%
  Kest(., correction="border") %>%
  plot()
#The correction="border" argument is specified for edge correction in the K-function.

# The Kpois(r) line in Red is the theoretical value of K for each distance window (r) under a Poisson assumption of Complete Spatial Randomness. 
# The Black line is the estimated values of K accounting for the effects of the edge of the study area.

#The correction specifies how points towards the edge are dealt with, in this case, border means that points towards the edge are ignored for the calculation but are included for the central points.
```
```{r}
# the value of K falls above the line, the data appear to be clustered at that distance.
# the value of K is below the line, the data are dispersed. 


# form the table
Kval <- as.data.frame(Kest(BluePlaquesSub.ppp, correction = "Ripley"))

```
## 6.6.5 Alternatives to Ripley’s K
## 6.7 Density-based spatial clustering of applications with noise: DBSCAN
```{r}
# Quadrat and Ripley’s K analysis are useful exploratory techniques for telling us if we have spatial clusters present in our point data, but they are not able to tell us WHERE in our area of interest the clusters are occurring.，但它们无法告诉我们聚类发生在我们感兴趣的区域中的何处。
# using DBSCAN
```
```{r}
install.packages("fpc")
```

```{r}
library(raster)
library(fpc)
```

```{r}
# first check the coordinate reference system of the Harrow spatial polygon:
st_geometry(BoroughMap)
```
```{r}
# DBSCAN requires you to input two parameters: 
# 1. Epsilon(eps) - this is the radius within which the algorithm with search for clusters 
# 2. MinPts(MinPts) - this is the minimum number of points that should be considered a cluster

#first extract the points from the spatial points data frame
BluePlaquesSubPoints <- BluePlaquesSub %>%
  coordinates(.)%>%
  as.data.frame()

#now run the dbscan analysis
db <- BluePlaquesSubPoints%>%
  fpc::dbscan(.,eps= 700, MinPts=4)

#now plot the results
plot(db, BluePlaquesSubPoints,main="DBSCAN output", frame = F)
plot (BoroughMap$geometry, add=T)
```
```{r}
# use kNNdistplot() from the dbscan pacakge to find a suitable eps value based on the ‘knee’ in the plot…
# 找到合适的eps的值
install.packages("dbscan")
library(dbscan)
```
```{r}
# used to find suitable eps value based on the knee in plot
# k is no of nearest neighbours used, use min points

BluePlaquesSubPoints%>%
  dbscan::kNNdistplot(.,k=4)
```
```{r}
library(ggplot2)
```

```{r}
db
```
```{r}
db$cluster
```
```{r}
BluePlaquesSubPoints <- BluePlaquesSubPoints %>%
  mutate(dbcluster = db$cluster)
#The values of this variable are assigned based on the cluster information obtained from the db object. Each point in BluePlaquesSubPoints will now have a corresponding cluster assignment in the dbcluster variable.
```

```{r}
print(class(db))
```
```{r}
#  to create some convex hull polygons to wrap around the points in our clusters. 创建一些凸面体（convex hull)多边形，以环绕我们的集群中的点。

chulls <- BluePlaquesSubPoints%>%
  group_by(dbcluster)%>%
  dplyr::mutate(hull=1:n(),
                hull= factor(hull,chull(coords.x1,coords.x2)))%>%
  arrange(hull)
# mutate(hull = 1:n()): Creates a new variable hull with values from 1 to the number of rows within each cluster.
# mutate(hull = factor(hull, chull(coords.x1, coords.x2))): Calculates the convex hull using the chull function for each cluster and converts the results to a factor.
# arrange(hull): Arranges the data frame based on the hull variable.
```

```{r}
# As 0 isn’t actually a cluster (it’s all points that aren’t in a cluster) drop it from the dataframe
chulls <- chulls %>%
  filter(dbcluster >=1)
```

```{r}
# Now create a ggplot2 object from our data
dbplot <- ggplot(data = BluePlaquesSubPoints,
                 aes(coords.x1,coords.x2, colour= dbcluster,fill=dbcluster))
#aes(coords.x1, coords.x2, colour = dbcluster, fill = dbcluster): Specifies the aesthetics for the plot. 
# coords.x1 and coords.x2 are the x and y coordinates
# colour = dbcluster and fill = dbcluster assign the color and fill aesthetics based on the dbcluster variable.
dbplot
```


```{r}
#add the points in
dbplot <- dbplot + geom_point()

# The geom_point() function is used to specify that points should be plotted based on the aesthetics you've defined earlier in the aes() function.

#now the convex hulls
dbplot <- dbplot + geom_polygon(data = chulls, 
                                aes(coords.x1,coords.x2, group=dbcluster), 
                                alpha = 0.5) 
#group = dbcluster indicates that the polygons are defined by the dbcluster variable.


#now plot, setting the coordinates to scale correctly and as a black and white plot 
#(just for the hell of it)...
dbplot + theme_bw() + coord_equal()

#theme_bw(): Sets the theme of the plot to a simple black and white theme. It provides a clean and clear background without any grid lines.将绘图主题设置为简单的黑白主题。它提供了一个干净清晰的背景，没有任何网格线。

#coord_equal(): Ensures that the x and y axes have the same scaling, creating a plot with equal aspect ratios. This is useful when dealing with spatial data to avoid distortion. 确保 x 轴和 y 轴具有相同的缩放比例，创建具有相同纵横比的绘图。 这在处理空间数据以避免失真时很有用。

# What is difference between ggplot2 and tmap?
# ggplot2:general-purpose data visualization with a focus on customization
# tmap: working with spatial data and want a simpler syntax for thematic mapping
```

```{r}
###add a basemap
##First get the bbox in lat long for Harrow
HarrowWGSbb <- Harrow %>%
  st_transform(., 4326)%>%
  st_bbox()
# The st_bbox() function from sf package is used to retrieve the bounding box (or bounding rectangle) of a spatial object.
```

```{r}
# convert the basemap to British National Grid
library(OpenStreetMap)
basemap <- OpenStreetMap::openmap(c(51.5549876,-0.4040502),c(51.6405356,-0.2671315),
                                  zoom=NULL,
                                  "osm")

#c(51.5549876, -0.4040502) and c(51.6405356, -0.2671315): These are the coordinates of the bounding box(边界框的坐标）. 
# The function will download the OpenStreetMap basemap data within this specified geographical extent.该函数将下载指定地理范围内的 OpenStreetMap 底图数据。

#zoom = NULL: This parameter allows you to specify a zoom level.该参数允许您指定缩放级别。 If set to NULL, the function will automatically determine the zoom level to fit the specified bounding box.

#"osm": Specifies that you want to use the OpenStreetMap (OSM) tile server as the source for the basemap.
```
```{r}
 # convert the basemap to British National Grid
basemap_bng <- openproj(basemap, projection="+init=epsg:27700")

# ‘openproj’ function from the OpenStreetMap 
# projection = "+init=epsg:27700": This specifies the target coordinate reference system (CRS) to which you want to reproject the basemap. 

# In this case, +init=epsg:27700 indicates the British National Grid (BNG) coordinate system with EPSG code 27700.
```

```{r}
#Now we can plot our fancy map with the clusters on…
#autoplot(basemap_bng) sometimes works
autoplot.OpenStreetMap(basemap_bng)+ 
  geom_point(data=BluePlaquesSubPoints, 
             aes(coords.x1,coords.x2, 
                 colour=dbcluster, 
                 fill=dbcluster)) + 
  geom_polygon(data = chulls, 
               aes(coords.x1,coords.x2, 
                   group=dbcluster,
                   fill=dbcluster), 
               alpha = 0.5) 
# the autoplot.OpenStreetMap() function from the OpenStreetMap package in R to create a plot of an OpenStreetMap basemap that has been reprojected to the British National Grid.
```
##6.8 Point pattern analysis summary

